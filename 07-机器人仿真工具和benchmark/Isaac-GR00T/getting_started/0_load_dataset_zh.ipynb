{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51b4cec",
   "metadata": {},
   "source": [
    "# 用于推理的数据集加载指南\n",
    "\n",
    "\n",
    "## LeRobot格式\n",
    "\n",
    "* 本教程将展示如何使用我们的数据加载器加载LeRobot格式的数据。\n",
    "* 我们将使用已经转换为LeRobot格式的`robot_sim.PickNPlace`数据集作为示例。\n",
    "* 要了解如何转换自己的数据集，请参考[LeRobot.md](LeRobot.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.dataset import ModalityConfig\n",
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f69c00",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "我们需要定义3个事项来加载数据集：\n",
    "1. 数据集的路径\n",
    "\n",
    "2. `ModalityConfigs`（模态配置）\n",
    "\n",
    "- `ModalityConfigs`定义了下游（如模型训练或推理）要使用哪些数据模态（如视频、状态、动作、语言）。\n",
    "- 每个模态通过delta_indices指定要加载哪一帧（例如[0]表示仅当前帧，[-1,0]表示前一帧和当前帧）\n",
    "\n",
    "3. `EmbodimentTag`（实施标签）\n",
    "- `EmbodimentTag`用于指定数据集的实施。所有实施标签的列表可以在`gr00t.data.embodiment_tags.EmbodimentTag`中找到。\n",
    "- GR00T的架构具有针对特定机器人类型（实施）优化的不同动作头。`EmbodimentTag`告诉模型在微调和/或推理时使用哪个动作头。在我们的例子中，由于我们使用的是人形机械臂，我们指定`EmbodimentTag.GR1_UNIFIED`以从人形特定动作头获得最佳性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gr00t\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "DATA_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
    "\n",
    "print(\"Loading dataset... from\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8014c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. modality configs\n",
    "modality_configs = {\n",
    "    \"video\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"video.ego_view\"],\n",
    "    ),\n",
    "    \"state\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"state.left_arm\",\n",
    "            \"state.left_hand\",\n",
    "            \"state.left_leg\",\n",
    "            \"state.neck\",\n",
    "            \"state.right_arm\",\n",
    "            \"state.right_hand\",\n",
    "            \"state.right_leg\",\n",
    "            \"state.waist\",\n",
    "        ],\n",
    "    ),\n",
    "    \"action\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"action.left_hand\",\n",
    "            \"action.right_hand\",\n",
    "        ],\n",
    "    ),\n",
    "    \"language\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"annotation.human.action.task_description\", \"annotation.human.validity\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. gr00t embodiment tag\n",
    "embodiment_tag = EmbodimentTag.GR1\n",
    "\n",
    "# load the dataset\n",
    "dataset = LeRobotSingleDataset(DATA_PATH, modality_configs,  embodiment_tag=embodiment_tag)\n",
    "\n",
    "print('\\n'*2)\n",
    "print(\"=\"*100)\n",
    "print(f\"{' Humanoid Dataset ':=^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9add6",
   "metadata": {},
   "source": [
    "显示数据中的图像帧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12991333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.ego_view\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5a1c6",
   "metadata": {},
   "source": [
    "## 转换数据\n",
    "\n",
    "我们还可以对我们的`LeRobotSingleDataset`类中的数据应用一系列转换。以下展示了如何对数据应用转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe728139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "\n",
    "\n",
    "video_modality = modality_configs[\"video\"]\n",
    "state_modality = modality_configs[\"state\"]\n",
    "action_modality = modality_configs[\"action\"]\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\"),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in state_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in action_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e9e17",
   "metadata": {},
   "source": [
    "现在看看应用转换后数据的变化。\n",
    "\n",
    "例如，状态和动作被归一化和连接，视频图像被裁剪、调整大小和颜色调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LeRobotSingleDataset(\n",
    "    DATA_PATH,\n",
    "    modality_configs,\n",
    "    transforms=to_apply_transforms,\n",
    "    embodiment_tag=embodiment_tag\n",
    ")\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
