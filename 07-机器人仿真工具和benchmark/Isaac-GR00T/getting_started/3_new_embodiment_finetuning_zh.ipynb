{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487104af",
   "metadata": {},
   "source": [
    "# 新实施微调教程\n",
    "\n",
    "本笔记本是关于如何在新数据集上微调GR00T-N1预训练模型的教程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777eb72",
   "metadata": {},
   "source": [
    "# 1. Lerobot SO100微调教程\n",
    "\n",
    "GR00T-N1对于各种机器人形态的每个人都是可访问的。基于Huggingface的低成本[So100 Lerobot手臂](https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md)，用户可以通过`new_embodiment`标签在自己的机器人上微调GR00T-N1。\n",
    "\n",
    "\n",
    "![so100_eval_demo.gif](../media/so100_eval_demo.gif)\n",
    "\n",
    "## 步骤1：数据集\n",
    "\n",
    "用户可以使用任何lerobot数据集进行微调。在本教程中，我们将首先使用一个示例数据集：[so100_strawberry_grape](https://huggingface.co/spaces/lerobot/visualize_dataset?dataset=youliangtan%2Fso100_strawberry_grape&episode=0)\n",
    "\n",
    "请注意，这种实施在我们的预训练数据集混合中没有使用过。\n",
    "\n",
    "\n",
    "### 首先，下载数据集\n",
    "\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset youliangtan/so100_strawberry_grape --local-dir ./demo_data/so100_strawberry_grape\n",
    "```\n",
    "\n",
    "### 其次，复制模态文件\n",
    "\n",
    "`modality.json`文件提供关于状态和动作模态的额外信息，使其\"GR00T兼容\"。将`examples/so100__modality.json`复制到数据集`<DATASET_PATH>/meta/modality.json`。\n",
    "\n",
    "```bash\n",
    "cp examples/so100__modality.json ./demo_data/so100_strawberry_grape/meta/modality.json\n",
    "```\n",
    "\n",
    "然后我们可以使用`LeRobotSingleDataset`类加载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "\n",
    "dataset_path = \"./demo_data/so100_strawberry_grape\"   # change this to your dataset path\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"so100\"]\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=data_config.modality_config(),\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n",
    "\n",
    "resp = dataset[7]\n",
    "any_describe(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdde0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.webcam\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f6681",
   "metadata": {},
   "source": [
    "## 步骤2：微调\n",
    "\n",
    "微调可以通过使用我们的微调脚本`scripts/gr00t_finetune.py`来完成。\n",
    "\n",
    "\n",
    "```bash\n",
    "python scripts/gr00t_finetune.py \\\n",
    "   --dataset-path /datasets/so100_strawberry_grape/ \\\n",
    "   --num-gpus 1 \\\n",
    "   --output-dir ~/so100-checkpoints  \\\n",
    "   --max-steps 2000 \\\n",
    "   --data-config so100 \\\n",
    "   --video-backend torchvision_av\n",
    "```\n",
    "\n",
    "## 步骤3：开环评估\n",
    "\n",
    "训练完成后，可以运行以下命令来可视化微调后的策略。\n",
    "\n",
    "```bash\n",
    "python scripts/eval_policy.py --plot \\\n",
    "   --embodiment_tag new_embodiment \\\n",
    "   --model_path <YOUR_CHECKPOINT_PATH> \\\n",
    "   --data_config so100 \\\n",
    "  --dataset_path /datasets/so100_strawberry_grape/ \\\n",
    "   --video_backend torchvision_av \\\n",
    "   --modality_keys single_arm gripper\n",
    "```\n",
    "\n",
    "这是训练策略7000步后的图表。\n",
    "\n",
    "![so100-7k-steps.png](../media/so100-7k-steps.png)\n",
    "\n",
    "\n",
    "经过更多步骤的训练后，图表会看起来明显更好。\n",
    "\n",
    "\n",
    "太棒了！已成功在新实施上微调了GR00T-N1。\n",
    "\n",
    "## 部署\n",
    "\n",
    "有关部署的更多详细信息，请参阅笔记本：`5_policy_deployment.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c2a7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. G1块堆叠数据集微调教程\n",
    "\n",
    "这提供了如何在G1块堆叠数据集上微调GR00T-N1的分步指南。\n",
    "\n",
    "## 步骤1：数据集\n",
    "\n",
    "加载用于微调的任何数据集可以通过2个步骤完成：\n",
    "- 1.1：为数据集定义模态配置和转换\n",
    "- 1.2：使用`LeRobotSingleDataset`类加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de821495",
   "metadata": {},
   "source": [
    "### 步骤：1.0 下载数据集\n",
    "\n",
    "- 从以下地址下载数据集：https://huggingface.co/datasets/unitreerobotics/G1_BlockStacking_Dataset\n",
    "- 将`examples/unitree_g1_blocks__modality.json`复制到数据集`<DATASET_PATH>/meta/modality.json`\n",
    "  - 这提供关于状态和动作模态的额外信息，使其\"GR00T兼容\"\n",
    "  - `cp examples/unitree_g1_blocks__modality.json datasets/G1_BlockStacking_Dataset/meta/modality.json`\n",
    "\n",
    "\n",
    "**理解模态配置**\n",
    "\n",
    "该文件提供有关状态和动作模态的详细元数据，使以下功能成为可能：\n",
    "\n",
    "- **分离数据存储和解释：**\n",
    "  - **状态和动作：**存储为连接的float32数组。`modality.json`文件提供了将这些数组解释为具有额外训练信息的不同、细粒度字段所需的元数据。\n",
    "  - **视频：**存储为单独的文件，配置文件允许将它们重命名为标准化格式。\n",
    "  - **注释：**跟踪所有注释字段。如果没有注释，请不要在配置文件中包含`annotation`字段。\n",
    "- **细粒度分割：**将状态和动作数组分为更具语义意义的字段。\n",
    "- **清晰映射：**数据维度的明确映射。\n",
    "- **复杂数据转换：**在训练期间支持特定字段的归一化和旋转转换。\n",
    "\n",
    "#### 模式\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"state\": {\n",
    "        \"<state_name>\": {\n",
    "            \"start\": <int>,         // 状态数组中的起始索引\n",
    "            \"end\": <int>,           // 状态数组中的结束索引\n",
    "        }\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"<action_name>\": {\n",
    "            \"start\": <int>,         // 动作数组中的起始索引\n",
    "            \"end\": <int>,           // 动作数组中的结束索引\n",
    "        }\n",
    "    },\n",
    "    \"video\": {\n",
    "        \"<video_name>\": {}  // 空字典，保持与其他模态的一致性\n",
    "    },\n",
    "    \"annotation\": {\n",
    "        \"<annotation_name>\": {}  // 空字典，保持与其他模态的一致性\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "示例在`getting_started/examples/unitree_g1_blocks__modality.json`中显示。此文件位于lerobot数据集的`meta`文件夹中。\n",
    "\n",
    "\n",
    "通过运行以下命令生成统计信息（`meta/metadata.json`）：\n",
    "```bash ```bash\n",
    "python scripts/load_dataset.py --data_path /datasets/G1_BlockStacking_Dataset/ --embodiment_tag new_embodiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b34097",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./demo_data/g1\"  # change this to your dataset path\n",
    "embodiment_tag = EmbodimentTag.NEW_EMBODIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43ab9b",
   "metadata": {},
   "source": [
    "### 步骤：1.1 模态配置和转换\n",
    "\n",
    "模态配置让可以选择在微调期间为每种输入类型（视频、状态、动作、语言等）使用哪些特定的数据流，让精确控制使用数据集的哪些部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4379813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import ModalityConfig\n",
    "\n",
    "\n",
    "# select the modality keys you want to use for finetuning\n",
    "video_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"video.cam_right_high\"],\n",
    ")\n",
    "\n",
    "state_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"state.left_arm\", \"state.right_arm\", \"state.left_hand\", \"state.right_hand\"],\n",
    ")\n",
    "\n",
    "action_modality = ModalityConfig(\n",
    "    delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    modality_keys=[\"action.left_arm\", \"action.right_arm\", \"action.left_hand\", \"action.right_hand\"],\n",
    ")\n",
    "\n",
    "language_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"annotation.human.task_description\"],\n",
    ")\n",
    "\n",
    "modality_configs = {\n",
    "    \"video\": video_modality,\n",
    "    \"state\": state_modality,\n",
    "    \"action\": action_modality,\n",
    "    \"language\": language_modality,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc970e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "from gr00t.model.transforms import GR00TTransform\n",
    "\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys, backend=\"torchvision\"),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95, backend=\"torchvision\"),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\", backend=\"torchvision\" ),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08, backend=\"torchvision\"),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            \"state.left_arm\": \"min_max\",\n",
    "            \"state.right_arm\": \"min_max\",\n",
    "            \"state.left_hand\": \"min_max\",\n",
    "            \"state.right_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            \"action.right_arm\": \"min_max\",\n",
    "            \"action.left_arm\": \"min_max\",\n",
    "            \"action.right_hand\": \"min_max\",\n",
    "            \"action.left_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "        # model-specific transform\n",
    "        GR00TTransform(\n",
    "            state_horizon=len(state_modality.delta_indices),\n",
    "            action_horizon=len(action_modality.delta_indices),\n",
    "            max_state_dim=64,\n",
    "            max_action_dim=32,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c328a7",
   "metadata": {},
   "source": [
    "### 步骤1.2 加载数据集\n",
    "\n",
    "首先我们将可视化数据集，然后使用`LeRobotSingleDataset`类加载它（不使用转换）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "\n",
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to visualize the images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "images = []\n",
    "for i in range(5):\n",
    "    image = train_dataset[i][\"video.cam_right_high\"][0]\n",
    "    # image is in HWC format, convert it to CHW format\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    images.append(image)   \n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i, image in enumerate(images):\n",
    "    axs[i].imshow(np.transpose(image, (1, 2, 0)))\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d15e69",
   "metadata": {},
   "source": [
    "现在，我们将使用我们的模态配置和转换初始化一个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    "    transforms=to_apply_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e83d9",
   "metadata": {},
   "source": [
    "**额外说明**：\n",
    " - 我们使用缓存数据加载器来加速训练速度。缓存数据加载器将所有数据加载到内存中，这显著提高了训练性能。但是，如果数据集很大或遇到内存不足（OOM）错误，可以切换到标准lerobot数据加载器（`gr00t.data.dataset.LeRobotSingleDataset`）。它使用与缓存数据加载器相同的API，因此可以在不更改代码的情况下来回切换。\n",
    " - 我们使用torchvision_av作为视频后端，视频编码采用av而不是标准h264。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc7af0",
   "metadata": {},
   "source": [
    "### 步骤2：加载模型\n",
    "\n",
    "训练过程分为3个步骤：\n",
    "- 2.1：从HuggingFace或本地路径加载基础模型\n",
    "- 2.2：准备训练参数\n",
    "- 2.3：运行训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bfb89",
   "metadata": {},
   "source": [
    "#### 步骤2.1 加载基础模型\n",
    "\n",
    "我们将使用`from_pretrained_for_tuning`方法加载模型。此方法允许我们指定要调整模型的哪些部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca380f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5072f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.model.gr00t_n1 import GR00T_N1\n",
    "\n",
    "BASE_MODEL_PATH = \"nvidia/GR00T-N1-2B\"\n",
    "TUNE_LLM = False            # Whether to tune the LLM\n",
    "TUNE_VISUAL = True          # Whether to tune the visual encoder\n",
    "TUNE_PROJECTOR = True       # Whether to tune the projector\n",
    "TUNE_DIFFUSION_MODEL = True # Whether to tune the diffusion model\n",
    "\n",
    "model = GR00T_N1.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL_PATH,\n",
    "    tune_llm=TUNE_LLM,  # backbone's LLM\n",
    "    tune_visual=TUNE_VISUAL,  # backbone's vision tower\n",
    "    tune_projector=TUNE_PROJECTOR,  # action head's projector\n",
    "    tune_diffusion_model=TUNE_DIFFUSION_MODEL,  # action head's DiT\n",
    ")\n",
    "\n",
    "# Set the model's compute_dtype to bfloat16\n",
    "model.compute_dtype = \"bfloat16\"\n",
    "model.config.compute_dtype = \"bfloat16\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0b0da",
   "metadata": {},
   "source": [
    "#### 步骤2.2 准备训练参数\n",
    "\n",
    "我们使用huggingface的`TrainingArguments`来配置训练过程。以下是主要参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c69e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"output/model/path\"    # CHANGE THIS ACCORDING TO YOUR LOCAL PATH\n",
    "per_device_train_batch_size = 8     # CHANGE THIS ACCORDING TO YOUR GPU MEMORY\n",
    "max_steps = 20                      # CHANGE THIS ACCORDING TO YOUR NEEDS\n",
    "report_to = \"wandb\"\n",
    "dataloader_num_workers = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=None,\n",
    "    remove_unused_columns=False,\n",
    "    deepspeed=\"\",\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.95,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10.0,\n",
    "    num_train_epochs=300,\n",
    "    max_steps=max_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=8,\n",
    "    report_to=report_to,\n",
    "    seed=42,\n",
    "    do_eval=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    ddp_bucket_cap_mb=100,\n",
    "    torch_compile_mode=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9482b2",
   "metadata": {},
   "source": [
    "#### 步骤2.3 初始化训练运行器并运行训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.runner import TrainRunner\n",
    "\n",
    "experiment = TrainRunner(\n",
    "    train_dataset=train_dataset,\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "experiment.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e487c",
   "metadata": {},
   "source": [
    "我们可以看到1000步离线验证结果与10000步离线验证结果的对比：\n",
    "\n",
    "**Unitree G1块堆叠数据集上的微调结果：**\n",
    "\n",
    "| 1k步 | 10k步 |\n",
    "| --- | --- |\n",
    "| ![1k](../media/g1_ft_1k.png) | ![10k](../media/g1_ft_10k.png) |\n",
    "| MSE: 0.0181 | MSE: 0.0022 | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
